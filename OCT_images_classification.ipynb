{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCT images classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXYTRiEntwvIF6VJckjNa6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sairamadithya/OCT-classification/blob/main/OCT_images_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing required libraries"
      ],
      "metadata": {
        "id": "xzH_1Ctk03cS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sMTH5NCm8a8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow\n",
        "from keras import Sequential\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (NORMAL,CNV,DME,DRUSEN). There are around 40k X-Ray images (JPEG) and 4 categories (NORMAL,CNV,DME,DRUSEN). \n",
        "\n",
        "This is a subset of the original dataset. More the training images, more the training time and we would require higher configurations of GPU. The common error associated with the training is OOM (Out Of Memory)\n",
        "\n",
        "Images are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL."
      ],
      "metadata": {
        "id": "eLDgf4eF09Z5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaK88Gbwm9LG",
        "outputId": "7ad9e4bb-2e86-4f92-e966-0ba5153dcb93"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jar xvf '/content/drive/MyDrive/OCT images.zip'"
      ],
      "metadata": {
        "id": "_BFOqBxmKrwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using imagedatagenerator to generate samples from the given image dataset by applying the mentioned transforms."
      ],
      "metadata": {
        "id": "W5rRJ30wajxg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klUlaZtmm_7N"
      },
      "source": [
        "train_dir= image.ImageDataGenerator(height_shift_range=0.1,width_shift_range=0.1,horizontal_flip=True)\n",
        "test_dir= image.ImageDataGenerator(height_shift_range=0.1,width_shift_range=0.1,horizontal_flip=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1wgH-ronCPk",
        "outputId": "c8c8b21d-5f34-484f-f0bc-132a1ea4d163"
      },
      "source": [
        "train_set= train_dir.flow_from_directory(\n",
        "    '/content/archive (1)/OCT2017/train',\n",
        "    batch_size=50,\n",
        "    class_mode='categorical',\n",
        "    target_size=(224,224)\n",
        ")\n",
        "test_set= test_dir.flow_from_directory(\n",
        "    '/content/archive (1)/OCT2017/test',\n",
        "    shuffle=False,\n",
        "    class_mode='categorical',\n",
        "    target_size=(224,224)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 40000 images belonging to 4 classes.\n",
            "Found 968 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xii7XJHnIUc",
        "outputId": "be7e1e3e-62e8-481c-f80f-7831e5e581b7"
      },
      "source": [
        "train_set.class_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CNV': 0, 'DME': 1, 'DRUSEN': 2, 'NORMAL': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have developed a transfer learning based MobileNetV2 model. I have added certain additional layers to overcome the problem of overfitting.\n",
        "\n",
        "GaussianNoise- this is a type of noise added to the data so as to increase the robustness and to introduce augmentation.\n",
        "\n",
        "dropout- this is used to reduce the model complexity by randomly cutting off certain nerual connections."
      ],
      "metadata": {
        "id": "QXDLwRAH2aMr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZaSOIRunMrR",
        "outputId": "e8145a70-a814-4c46-94fc-2f8452d29a8f"
      },
      "source": [
        "base_model = tensorflow.keras.applications.MobileNetV2(weights='imagenet', input_shape=(224,224,3), include_top=False)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "model=Sequential()\n",
        "model.add(base_model)\n",
        "model.add(GaussianNoise(0.25))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(GaussianNoise(0.25))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(4,activation='softmax'))\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " gaussian_noise (GaussianNoi  (None, 7, 7, 1280)       0         \n",
            " se)                                                             \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 7, 7, 1280)       5120      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 62720)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                4014144   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " gaussian_noise_1 (GaussianN  (None, 64)               0         \n",
            " oise)                                                           \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,281,924\n",
            "Trainable params: 6,245,124\n",
            "Non-trainable params: 36,800\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model is compiled using adagrad optimizer, crossentropy loss function and metrics i have used is accuracy. You can add AUC, precision, recall and other classification metrics."
      ],
      "metadata": {
        "id": "SLaNmORhkTyz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4gjZptjnOqJ"
      },
      "source": [
        "model.compile(optimizer='AdaGrad',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a very good dataset and training for less epochs gives good results. I trained the model for 10 epochs with 80 steps per each epoch."
      ],
      "metadata": {
        "id": "c2sEA6pxk9Oo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8oYJet4nSPJ",
        "outputId": "f251535f-4da0-4972-85bb-33a538ca2185"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "es=EarlyStopping(monitor='val_loss',patience=5)\n",
        "history=model.fit(\n",
        "    train_set,\n",
        "    epochs=10,\n",
        "    steps_per_epoch= 80,\n",
        "    validation_data= test_set,\n",
        "    callbacks=es\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "80/80 [==============================] - 89s 1s/step - loss: 0.9167 - accuracy: 0.6425 - val_loss: 1.2723 - val_accuracy: 0.5258\n",
            "Epoch 2/10\n",
            "80/80 [==============================] - 84s 1s/step - loss: 0.5887 - accuracy: 0.7845 - val_loss: 1.2294 - val_accuracy: 0.5671\n",
            "Epoch 3/10\n",
            "80/80 [==============================] - 82s 1s/step - loss: 0.4795 - accuracy: 0.8342 - val_loss: 0.9612 - val_accuracy: 0.6374\n",
            "Epoch 4/10\n",
            "80/80 [==============================] - 83s 1s/step - loss: 0.4250 - accuracy: 0.8580 - val_loss: 0.6209 - val_accuracy: 0.7696\n",
            "Epoch 5/10\n",
            "80/80 [==============================] - 83s 1s/step - loss: 0.3903 - accuracy: 0.8683 - val_loss: 0.4146 - val_accuracy: 0.8481\n",
            "Epoch 6/10\n",
            "80/80 [==============================] - 82s 1s/step - loss: 0.3594 - accuracy: 0.8780 - val_loss: 0.3269 - val_accuracy: 0.8781\n",
            "Epoch 7/10\n",
            "80/80 [==============================] - 82s 1s/step - loss: 0.3493 - accuracy: 0.8892 - val_loss: 0.3598 - val_accuracy: 0.8781\n",
            "Epoch 8/10\n",
            "80/80 [==============================] - 82s 1s/step - loss: 0.3263 - accuracy: 0.8928 - val_loss: 0.2603 - val_accuracy: 0.9143\n",
            "Epoch 9/10\n",
            "80/80 [==============================] - 81s 1s/step - loss: 0.3303 - accuracy: 0.8903 - val_loss: 0.2258 - val_accuracy: 0.9329\n",
            "Epoch 10/10\n",
            "80/80 [==============================] - 82s 1s/step - loss: 0.3095 - accuracy: 0.8982 - val_loss: 0.1805 - val_accuracy: 0.9473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model achieved the following results:-\n",
        "training accuracy- 78%\n",
        "\n",
        "training loss- 0.598\n",
        "\n",
        "validation accuracy- 94.11%\n",
        "\n",
        "validation loss- 0.186"
      ],
      "metadata": {
        "id": "MX9rpjuzmX_W"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGVXiNeUnWL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c56e10-7670-4997-8714-92ce23404108"
      },
      "source": [
        "model.evaluate(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800/800 [==============================] - 556s 695ms/step - loss: 0.5985 - accuracy: 0.7876\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5985212922096252, 0.7876250147819519]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYfFUWbBnc_W",
        "outputId": "3f2f1a32-9fdb-4f8c-9611-869fc4e6980d"
      },
      "source": [
        "model.evaluate(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31/31 [==============================] - 14s 440ms/step - loss: 0.1868 - accuracy: 0.9411\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18679122626781464, 0.94111567735672]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.models.save_model(model,'mymodel.hdf5')"
      ],
      "metadata": {
        "id": "ppsbYWQOPj3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have developed a website that asks the OCT image from the user in the form of JPG, PNG or JPEG. Once that is uploaded, it will classify into what the eye disease is present in that OCT image."
      ],
      "metadata": {
        "id": "3gKNYGpJo4AM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UIWc4XLRnyVV",
        "outputId": "22ffbcee-50ec-459b-955c-59037b822f02"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.7.0-py2.py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.21.5)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.3.5)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 30.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.11.2)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.13.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.10.0.2)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 44.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.3)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Collecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.0.1)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.4.0)\n",
            "Collecting blinker\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.4)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->streamlit) (3.7.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.1.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.5)\n",
            "Collecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.9.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.3)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Collecting ipython>=7.23.1\n",
            "  Downloading ipython-7.32.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 41.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.5.4)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.4.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.28-py3-none-any.whl (380 kB)\n",
            "\u001b[K     |████████████████████████████████| 380 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.9.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.13.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (3.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Building wheels for collected packages: blinker\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=829ac1724a1c1b9eabb11487c7e154ce30ec964db9bf81bf7a913dece98d9c10\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "Successfully built blinker\n",
            "Installing collected packages: prompt-toolkit, ipython, ipykernel, smmap, gitdb, watchdog, validators, toml, pympler, pydeck, gitpython, blinker, base58, streamlit\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.9.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.32.0 which is incompatible.\u001b[0m\n",
            "Successfully installed base58-2.1.1 blinker-1.4 gitdb-4.0.9 gitpython-3.1.27 ipykernel-6.9.1 ipython-7.32.0 prompt-toolkit-3.0.28 pydeck-0.7.1 pympler-1.0.1 smmap-5.0.0 streamlit-1.7.0 toml-0.10.2 validators-0.18.2 watchdog-2.1.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF7oQeoIoDS_",
        "outputId": "3295b47d-b63e-475a-fe82-a1e19b6851a8"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def load_model():\n",
        "  model=tf.keras.models.load_model('/content/mymodel.hdf5')\n",
        "  return model\n",
        "with st.spinner('Model is being loaded..'):\n",
        "  model=load_model()\n",
        "\n",
        "st.write(\"\"\"\n",
        "         # OCT IMAGE DIAGNOSIS USING DEEP LEARNING\n",
        "           by V A SAIRAM \n",
        "         \"\"\"\n",
        ")\n",
        "file = st.file_uploader(\"Please upload any image from the local machine in case of computer or upload camera image in case of mobile\", type=[\"jpg\", \"png\",\"jpeg\",\"webp\"])\n",
        "import cv2\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "st.set_option('deprecation.showfileUploaderEncoding', False)\n",
        "def import_and_predict(image_data, model):\n",
        "    \n",
        "        size = (224,224)    \n",
        "        image = ImageOps.fit(image_data, size, Image.ANTIALIAS)\n",
        "        image = np.asarray(image)\n",
        "        img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        img_reshape = img[np.newaxis,...]\n",
        "    \n",
        "        prediction = model.predict(img_reshape)\n",
        "        return prediction\n",
        "if file is None:\n",
        "    st.text(\"Please upload an image file within the allotted file size or retry of the error still persists...\")\n",
        "else:\n",
        "    image = Image.open(file)\n",
        "    st.image(image, use_column_width=True)\n",
        "    predictions = import_and_predict(image, model)\n",
        "    class_names= ['CNV', \n",
        "                  'DME',\n",
        "                  'DRUSEN',\n",
        "                  'NORMAL'\n",
        "                  ]\n",
        "    string= \"THE PARTICULAR WASTE CATEGORY IS: \" + class_names[np.argmax(predictions)]\n",
        "    st.success(string) \n",
        "    #string1=\"Most of the wastes cannot be disposed at our homes. There are special collection sites for the disposal. Our work is to isolate the wastes into seperate containers which can be send to the collection sites for proper disposal.\"  \n",
        "    string2=\"HOPE THIS HELPS!! THANK YOU FOR USING THIS PROJECT!!!\"\n",
        "    st.success(string2)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "HI4tIR2fQOs7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aad904c-856b-48dc-950f-20be40270f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-11 06:19:57.098 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "\u001b[K\u001b[?25h\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.225.194.61:8501\u001b[0m\n",
            "\u001b[0m\n",
            "npx: installed 22 in 5.756s\n",
            "your url is: https://massive-bear-38.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vGj1rqWWmUyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}